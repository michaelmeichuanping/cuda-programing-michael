{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f28404c3-4ca5-4427-becb-1cdbadaf3ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from numba import jit\n",
    "from numba import cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c5003f-21f1-446f-bfa2-417d2c42d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@cuda.jit\n",
    "def mutiply(arr_A, arr_B, arr_C):\n",
    "    # (row, col) 为当前thread的横坐标和纵坐标\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < arr_C.shape[0] and col < arr_C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(arr_A.shape[1]):\n",
    "            tmp += arr_A[row, k] * arr_B[k, col]\n",
    "        arr_C[row, col] = tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d5a464c-4538-49d0-bc2c-2bee0268e307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU time required:  3.2184762954711914\n",
      "gpu result correct\n"
     ]
    }
   ],
   "source": [
    "M = 6000\n",
    "N = 4800\n",
    "P = 4000\n",
    "A = np.random.random((M, N)) # 随机生成的 [M x N] 矩阵\n",
    "B = np.random.random((N, P)) # 随机生成的 [N x P] 矩阵\n",
    "C_gpu = np.zeros((M, P))\n",
    "# manually copy numpy matrices from the main memory to the GPU global memory\n",
    "A_global_mem = cuda.to_device(A)\n",
    "B_global_mem = cuda.to_device(B)\n",
    "\n",
    "# 在显卡设备上初始化一块GPU memory, for storing GPU计算结果, 以避免结果被回送到CPU\n",
    "C_global_mem = cuda.device_array((A.shape[0], B.shape[1]))\n",
    "\n",
    "\n",
    "# execute configuration\n",
    "threads_per_block = (8, 16)\n",
    "blocks_per_grid_x = int(math.ceil(A.shape[0] / threads_per_block[0]))\n",
    "blocks_per_grid_y = int(math.ceil(B.shape[1] / threads_per_block[1]))\n",
    "blocksPerGrid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "# mutiply[blocksPerGrid, threads_per_block](A, B, C_gpu)\n",
    "mutiply[blocksPerGrid, threads_per_block](A_global_mem, B_global_mem, C_global_mem)\n",
    "cuda.synchronize()\n",
    "\n",
    "print(\"GPU time required:  %s\" %(time.time() - start))\n",
    "\n",
    "# Copy the result matrix back to the host\n",
    "C_gpu = C_global_mem.copy_to_host()\n",
    "\n",
    "# 验证正确性\n",
    "if np.allclose(C_gpu, np.dot(A, B)):\n",
    "    print(\"gpu result correct\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba59da80-7f49-4352-bf73-1fdbd42240ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46808a18-cb2a-4776-bd2c-0b788bb4aa3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-3-11_programing",
   "language": "python",
   "name": "py-3-11_programing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
